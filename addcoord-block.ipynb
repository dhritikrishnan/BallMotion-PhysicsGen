{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13480612,"sourceType":"datasetVersion","datasetId":8558578},{"sourceId":13853689,"sourceType":"datasetVersion","datasetId":8825027}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SZ","metadata":{}},{"cell_type":"code","source":"\"\"\"\nPix2Pix Implementation for PhysicsGen Motion Prediction\nBased on paper appendix - Table 5 hyperparameters\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport time\nfrom pathlib import Path\nimport itertools\n\ntry:\n    import wandb\nexcept ImportError:\n    print(\"Installing wandb...\")\n    !pip install wandb -q\n    import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:44.257362Z","iopub.execute_input":"2025-12-02T16:21:44.258262Z","iopub.status.idle":"2025-12-02T16:21:44.264611Z","shell.execute_reply.started":"2025-12-02T16:21:44.258222Z","shell.execute_reply":"2025-12-02T16:21:44.263779Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ['WANDB_API_KEY'] = 'f3579be751c0a30268d1d947d8e79f9ea3486905'\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:44.266026Z","iopub.execute_input":"2025-12-02T16:21:44.266321Z","iopub.status.idle":"2025-12-02T16:21:51.649411Z","shell.execute_reply.started":"2025-12-02T16:21:44.266302Z","shell.execute_reply":"2025-12-02T16:21:51.648849Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhritik\u001b[0m (\u001b[33mdhritik-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class CombinedImageDataset(Dataset):\n    def __init__(self, data_dir, transform=None, split_direction='horizontal'):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.split_direction = split_direction\n        self.image_files = sorted([\n            f for f in os.listdir(self.data_dir) \n            if f.endswith(('.png', '.jpg', '.jpeg'))\n        ])\n        \n        print(f\"{len(self.image_files)} combined images from {data_dir}\")\n        \n        if self.image_files:\n            first_img = Image.open(self.data_dir / self.image_files[0])\n            w, h = first_img.size\n            print(f\"  Image size: {w}x{h}\")\n            \n            if split_direction == 'horizontal':\n                if w > h * 1.5:\n                    print(f\"  Detected horizontal split (width > height)\")\n                    print(f\"  Will split into: {w//2}x{h} (left) and {w//2}x{h} (right)\")\n                else:\n                    print(f\"  Image might not be horizontally combined\")\n            else:\n                if h > w * 1.5:\n                    print(f\"  ✓ Detected vertical split (height > width)\")\n                    print(f\"  Will split into: {w}x{h//2} (top) and {w}x{h//2} (bottom)\")\n                else:\n                    print(f\"  Image might not be vertically combined\")\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.data_dir / self.image_files[idx]\n        combined_img = Image.open(img_path).convert('RGB')\n        \n        w, h = combined_img.size\n        \n        if self.split_direction == 'horizontal':\n            mid = w // 2\n            input_img = combined_img.crop((0, 0, mid, h))\n            target_img = combined_img.crop((mid, 0, w, h))\n        else:\n            mid = h // 2\n            input_img = combined_img.crop((0, 0, w, mid))\n            target_img = combined_img.crop((0, mid, w, h))\n        \n        if self.transform:\n            input_img = self.transform(input_img)\n            target_img = self.transform(target_img)\n        \n        return input_img, target_img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.650247Z","iopub.execute_input":"2025-12-02T16:21:51.650718Z","iopub.status.idle":"2025-12-02T16:21:51.659201Z","shell.execute_reply.started":"2025-12-02T16:21:51.650689Z","shell.execute_reply":"2025-12-02T16:21:51.658374Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def create_data_loaders(data_root, batch_size=18, image_size=256, num_workers=2, \n                       split_direction='horizontal'):\n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    \n    print(f\"Split direction: {split_direction}\")\n    \n    \n    train_path = Path(data_root) / 'train_double'\n    print(\"\\nTrain set:\")\n    train_dataset = CombinedImageDataset(\n        str(train_path), \n        transform=transform,\n        split_direction=split_direction\n    )\n    \n    val_path = Path(data_root) / 'val_double'\n    test_path = Path(data_root) / 'test_double'\n    \n    if val_path.exists():\n        print(\"\\nValidation set:\")\n        val_dataset = CombinedImageDataset(\n            str(val_path), \n            transform=transform,\n            split_direction=split_direction\n        )\n    else:\n        print(\"\\nValidation set: Creating 10% split from training\")\n        train_size = int(0.9 * len(train_dataset))\n        val_size = len(train_dataset) - train_size\n        train_dataset, val_dataset = torch.utils.data.random_split(\n            train_dataset, [train_size, val_size]\n        )\n    \n    if test_path.exists():\n        print(\"\\nTest set:\")\n        test_dataset = CombinedImageDataset(\n            str(test_path), \n            transform=transform,\n            split_direction=split_direction\n        )\n    else:\n        print(\"\\nTest set: Using val set\")\n        test_dataset = val_dataset\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n                             shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n                           shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n                            shuffle=False, num_workers=num_workers)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.659915Z","iopub.execute_input":"2025-12-02T16:21:51.660161Z","iopub.status.idle":"2025-12-02T16:21:51.680123Z","shell.execute_reply.started":"2025-12-02T16:21:51.660133Z","shell.execute_reply":"2025-12-02T16:21:51.679364Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels, affine=True))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.681756Z","iopub.execute_input":"2025-12-02T16:21:51.682004Z","iopub.status.idle":"2025-12-02T16:21:51.701588Z","shell.execute_reply.started":"2025-12-02T16:21:51.681986Z","shell.execute_reply":"2025-12-02T16:21:51.700933Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class AddCoords(nn.Module):\n    \"\"\"\n    Adds two extra channels (X and Y coordinates) to the input tensor.\n    This gives the network 'spatial awareness', which is crucial for\n    physics tasks like gravity or boundary detection.\n    \"\"\"\n    def __init__(self, with_r=False):\n        super().__init__()\n        self.with_r = with_r\n\n    def forward(self, input_tensor):\n        \"\"\"\n        Args:\n            input_tensor: shape (batch, channel, x_dim, y_dim)\n        \"\"\"\n        batch_size, _, x_dim, y_dim = input_tensor.size()\n\n        \n        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n\n      \n        xx_channel = xx_channel.float() / (x_dim - 1)\n        yy_channel = yy_channel.float() / (y_dim - 1)\n        xx_channel = xx_channel * 2 - 1\n        yy_channel = yy_channel * 2 - 1\n\n       \n        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n\n       \n        xx_channel = xx_channel.type_as(input_tensor)\n        yy_channel = yy_channel.type_as(input_tensor)\n\n        ret = torch.cat([input_tensor, xx_channel, yy_channel], dim=1)\n\n        if self.with_r:\n            rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2))\n            ret = torch.cat([ret, rr], dim=1)\n\n        return ret","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.702386Z","iopub.execute_input":"2025-12-02T16:21:51.702646Z","iopub.status.idle":"2025-12-02T16:21:51.720519Z","shell.execute_reply.started":"2025-12-02T16:21:51.702618Z","shell.execute_reply":"2025-12-02T16:21:51.719841Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(Generator, self).__init__()\n\n\n        self.add_coords = AddCoords(with_r=False)\n        self.down1 = UNetDown(in_channels + 2, 64, normalize=False) \n        self.down2 = UNetDown(64, 128)                          \n        self.down3 = UNetDown(128, 256)                       \n        self.down4 = UNetDown(256, 512)          \n        self.down5 = UNetDown(512, 512)\n        self.down6 = UNetDown(512, 512)\n        self.down7 = UNetDown(512, 512)\n        self.down8 = UNetDown(512, 512, normalize=False) \n\n        \n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5) \n        self.up4 = UNetUp(1024, 512)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128) \n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n\n        x = self.add_coords(x)\n        \n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        \n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.721221Z","iopub.execute_input":"2025-12-02T16:21:51.721451Z","iopub.status.idle":"2025-12-02T16:21:51.739181Z","shell.execute_reply.started":"2025-12-02T16:21:51.721425Z","shell.execute_reply":"2025-12-02T16:21:51.738505Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, in_channels=6):\n        super(Discriminator, self).__init__()\n        self.add_coords = AddCoords(with_r=False)\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters, affine=True))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels+2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )\n\n    def forward(self, img_A, img_B):\n        img_input = torch.cat((img_A, img_B), 1)\n        img_input = self.add_coords(img_input)\n        return self.model(img_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.739902Z","iopub.execute_input":"2025-12-02T16:21:51.740170Z","iopub.status.idle":"2025-12-02T16:21:51.759024Z","shell.execute_reply.started":"2025-12-02T16:21:51.740143Z","shell.execute_reply":"2025-12-02T16:21:51.758207Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def compute_gradient_penalty(discriminator, real_A, real_B, fake_B, device, lambda_gp=10):\n    \n    batch_size = real_B.size(0)\n    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n    \n    interpolates = (alpha * real_B + (1 - alpha) * fake_B).requires_grad_(True)\n    d_interpolates = discriminator(real_A, interpolates)\n    fake = torch.ones(d_interpolates.size(), device=device, requires_grad=False)\n    \n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_gp\n    \n    return gradient_penalty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.759916Z","iopub.execute_input":"2025-12-02T16:21:51.760201Z","iopub.status.idle":"2025-12-02T16:21:51.777303Z","shell.execute_reply.started":"2025-12-02T16:21:51.760176Z","shell.execute_reply":"2025-12-02T16:21:51.776593Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_epoch(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n                criterion_GAN, criterion_L1, lambda_L1, lambda_gp, device, epoch):\n    generator.train()\n    discriminator.train()\n    \n    epoch_g_loss = 0\n    epoch_d_loss = 0\n    epoch_gan_loss = 0\n    epoch_l1_loss = 0\n    epoch_gp_loss = 0 \n    \n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} - Training')\n    for batch_idx, (real_A, real_B) in enumerate(pbar):\n        real_A = real_A.to(device)\n        real_B = real_B.to(device)\n        \n        batch_size = real_A.size(0)\n        valid = torch.ones((batch_size, 1, 16, 16), device=device, requires_grad=False)\n        fake = torch.zeros((batch_size, 1, 16, 16), device=device, requires_grad=False)\n        \n        optimizer_G.zero_grad()\n        fake_B = generator(real_A)\n        pred_fake = discriminator(real_A, fake_B)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        loss_L1 = criterion_L1(fake_B, real_B)\n        loss_G = loss_GAN + lambda_L1 * loss_L1\n        loss_G.backward()\n        optimizer_G.step()\n        \n        optimizer_D.zero_grad()\n        \n        pred_real = discriminator(real_A, real_B)\n        loss_real = criterion_GAN(pred_real, valid)\n        \n        pred_fake = discriminator(real_A, fake_B.detach())\n        loss_fake = criterion_GAN(pred_fake, fake)\n        \n        gp = compute_gradient_penalty(discriminator, real_A, real_B, \n                                     fake_B.detach(), device, lambda_gp)\n        \n        loss_D = 0.5 * (loss_real + loss_fake) + gp\n        loss_D.backward()\n        optimizer_D.step()\n        \n        epoch_g_loss += loss_G.item()\n        epoch_d_loss += loss_D.item()\n        epoch_gan_loss += loss_GAN.item()\n        epoch_l1_loss += loss_L1.item()\n        epoch_gp_loss += gp.item()\n\n        if batch_idx % 10 == 0:\n            wandb.log({\n                'batch/g_loss': loss_G.item(),\n                'batch/d_loss': loss_D.item(),\n                'batch/gan_loss': loss_GAN.item(),\n                'batch/l1_loss': loss_L1.item(),\n                'batch/gp_loss': gp.item(),\n                'batch/step': epoch * len(train_loader) + batch_idx\n            })\n        \n        pbar.set_postfix({\n            'G': f'{loss_G.item():.4f}',\n            'D': f'{loss_D.item():.4f}',\n            'L1': f'{loss_L1.item():.4f}',\n            'GP': f'{gp.item():.4f}'\n        })\n    \n    num_batches = len(train_loader)\n    return (epoch_g_loss/num_batches, epoch_d_loss/num_batches,\n            epoch_gan_loss/num_batches, epoch_l1_loss/num_batches,\n            epoch_gp_loss/num_batches)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.777967Z","iopub.execute_input":"2025-12-02T16:21:51.778217Z","iopub.status.idle":"2025-12-02T16:21:51.792939Z","shell.execute_reply.started":"2025-12-02T16:21:51.778191Z","shell.execute_reply":"2025-12-02T16:21:51.792347Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def validate(generator, discriminator, val_loader, criterion_GAN, \n             criterion_L1, lambda_L1, device, epoch):\n    generator.eval()\n    discriminator.eval()\n    \n    val_g_loss = 0\n    val_d_loss = 0\n    \n    with torch.no_grad():\n        for real_A, real_B in tqdm(val_loader, desc=f'Epoch {epoch+1} - Validation'):\n            real_A = real_A.to(device)\n            real_B = real_B.to(device)\n            \n            batch_size = real_A.size(0)\n            valid = torch.ones((batch_size, 1, 16, 16), device=device)\n            fake = torch.zeros((batch_size, 1, 16, 16), device=device)\n            \n            fake_B = generator(real_A)\n            pred_fake = discriminator(real_A, fake_B)\n            loss_GAN = criterion_GAN(pred_fake, valid)\n            loss_L1 = criterion_L1(fake_B, real_B)\n            loss_G = loss_GAN + lambda_L1 * loss_L1\n            \n            pred_real = discriminator(real_A, real_B)\n            loss_real = criterion_GAN(pred_real, valid)\n            pred_fake = discriminator(real_A, fake_B)\n            loss_fake = criterion_GAN(pred_fake, fake)\n            loss_D = 0.5 * (loss_real + loss_fake)\n            \n            val_g_loss += loss_G.item()\n            val_d_loss += loss_D.item()\n    \n    num_batches = len(val_loader)\n    return val_g_loss/num_batches, val_d_loss/num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.793629Z","iopub.execute_input":"2025-12-02T16:21:51.793893Z","iopub.status.idle":"2025-12-02T16:21:51.812034Z","shell.execute_reply.started":"2025-12-02T16:21:51.793877Z","shell.execute_reply":"2025-12-02T16:21:51.811316Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_pix2pix(generator, discriminator, train_loader, val_loader, \n                  config, device, save_dir='checkpoints'):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    optimizer_G = optim.Adam(generator.parameters(), \n                            lr=config['lr_generator'], \n                            betas=(0.5, 0.999))\n    optimizer_D = optim.Adam(discriminator.parameters(), \n                            lr=config['lr_discriminator'], \n                            betas=(0.5, 0.999))\n    \n    criterion_GAN = nn.MSELoss()\n    criterion_L1 = nn.L1Loss()\n    \n    best_val_loss = float('inf')\n    print(\"Starting Pix2Pix Training with Gradient Penalty\")\n    print(\"=\"*70)\n    print(f\"Lambda L1: {config['lambda_l1']}\")\n    print(f\"Lambda GP: {config['lambda_gp']}\")  # Show GP weight\n    print(\"=\"*70)\n    \n    start_time = time.time()\n    \n    for epoch in range(config['num_epochs']):\n        epoch_start = time.time()\n        \n        # Train with gradient penalty\n        train_g, train_d, train_gan, train_l1, train_gp = train_epoch(\n            generator, discriminator, train_loader,\n            optimizer_G, optimizer_D,\n            criterion_GAN, criterion_L1, \n            config['lambda_l1'], config['lambda_gp'],  # Pass lambda_gp\n            device, epoch\n        )\n        \n        # Validate\n        val_g, val_d = validate(\n            generator, discriminator, val_loader,\n            criterion_GAN, criterion_L1, config['lambda_l1'],\n            device, epoch\n        )\n        \n        epoch_time = time.time() - epoch_start\n        \n        # Log to wandb (including GP)\n        wandb.log({\n            'epoch': epoch + 1,\n            'train/g_loss': train_g,\n            'train/d_loss': train_d,\n            'train/gan_loss': train_gan,\n            'train/l1_loss': train_l1,\n            'train/gp_loss': train_gp,  # Log gradient penalty\n            'val/g_loss': val_g,\n            'val/d_loss': val_d,\n            'epoch_time': epoch_time\n        })\n        \n        print(f\"\\nEpoch [{epoch+1}/{config['num_epochs']}] | Time: {epoch_time:.1f}s\")\n        print(f\"  Train - G: {train_g:.4f}, D: {train_d:.4f}, GP: {train_gp:.4f}\")\n        print(f\"  Val   - G: {val_g:.4f}, D: {val_d:.4f}\")\n        \n        if val_g < best_val_loss:\n            best_val_loss = val_g\n            checkpoint = {\n                'epoch': epoch,\n                'generator_state_dict': generator.state_dict(),\n                'discriminator_state_dict': discriminator.state_dict(),\n                'val_loss': val_g,\n            }\n            torch.save(checkpoint, os.path.join(save_dir, 'best_pix2pix.pth'))\n            wandb.save(os.path.join(save_dir, 'best_pix2pix.pth'))\n            print(f\"  ✓ Saved best model\")\n        \n        # Visualize every 10 epochs\n        if (epoch + 1) % 10 == 0:\n            visualize_results(generator, val_loader, device, epoch)\n    \n    total_time = time.time() - start_time\n    wandb.run.summary['best_val_loss'] = best_val_loss\n    wandb.run.summary['total_training_time'] = total_time\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"Training completed in {total_time/3600:.2f} hours\")\n    print(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.812831Z","iopub.execute_input":"2025-12-02T16:21:51.813048Z","iopub.status.idle":"2025-12-02T16:21:51.826820Z","shell.execute_reply.started":"2025-12-02T16:21:51.813032Z","shell.execute_reply":"2025-12-02T16:21:51.826172Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def visualize_results(generator, val_loader, device, epoch):\n    generator.eval()\n    \n    with torch.no_grad():\n        real_A, real_B = next(iter(val_loader))\n        real_A = real_A.to(device)\n        fake_B = generator(real_A)\n        \n        def denorm(x):\n            return (x + 1) / 2\n        \n        num_vis = min(4, len(real_A))\n        fig, axes = plt.subplots(3, num_vis, figsize=(4*num_vis, 12))\n        \n        for i in range(num_vis):\n            input_img = denorm(real_A[i]).cpu().permute(1, 2, 0).numpy()\n            axes[0, i].imshow(input_img)\n            axes[0, i].set_title(f'Input {i+1}')\n            axes[0, i].axis('off')\n            \n            gen_img = denorm(fake_B[i]).cpu().permute(1, 2, 0).numpy()\n            axes[1, i].imshow(gen_img)\n            axes[1, i].set_title(f'Generated {i+1}')\n            axes[1, i].axis('off')\n            \n            target_img = denorm(real_B[i]).cpu().permute(1, 2, 0).numpy()\n            axes[2, i].imshow(target_img)\n            axes[2, i].set_title(f'Ground Truth {i+1}')\n            axes[2, i].axis('off')\n        \n        plt.tight_layout()\n        wandb.log({f\"generations/epoch_{epoch+1}\": wandb.Image(fig), \"epoch\": epoch + 1})\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.827479Z","iopub.execute_input":"2025-12-02T16:21:51.827707Z","iopub.status.idle":"2025-12-02T16:21:51.844016Z","shell.execute_reply.started":"2025-12-02T16:21:51.827684Z","shell.execute_reply":"2025-12-02T16:21:51.843371Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class BallDetector:\n    \n    def __init__(self, ball_radius=15, min_radius=10, max_radius=25):\n        self.ball_radius = ball_radius\n        self.min_radius = min_radius\n        self.max_radius = max_radius\n    \n    def detect_ball(self, image):\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n        \n        blurred = cv2.GaussianBlur(gray, (9, 9), 2)\n        circles = cv2.HoughCircles(\n            blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=50,\n            param1=50, param2=30,\n            minRadius=self.min_radius, maxRadius=self.max_radius\n        )\n        \n        if circles is None or len(circles[0]) == 0:\n            return None\n        \n        circle = circles[0][0]\n        return {'position': (circle[0], circle[1]), 'radius': circle[2]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.846164Z","iopub.execute_input":"2025-12-02T16:21:51.846341Z","iopub.status.idle":"2025-12-02T16:21:51.861939Z","shell.execute_reply.started":"2025-12-02T16:21:51.846328Z","shell.execute_reply":"2025-12-02T16:21:51.861357Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def evaluate_pix2pix(generator, test_loader, device):\n    generator.eval()\n    detector = BallDetector()\n    \n    all_errors_x = []\n    all_errors_y = []\n    failed_predictions = 0\n    total_predictions = 0\n    sample_predictions = []\n    \n    print(\"Evaluating Pix2Pix Model\")\n    \n    with torch.no_grad():\n        for batch_idx, (real_A, real_B) in enumerate(tqdm(test_loader, desc='Evaluating')):\n            real_A = real_A.to(device)\n            fake_B = generator(real_A)\n            \n            fake_imgs = ((fake_B + 1) / 2 * 255).cpu().numpy().astype(np.uint8)\n            real_imgs = ((real_B + 1) / 2 * 255).numpy().astype(np.uint8)\n            \n            for i in range(len(fake_imgs)):\n                total_predictions += 1\n                \n                pred = fake_imgs[i].transpose(1, 2, 0)\n                target = real_imgs[i].transpose(1, 2, 0)\n                \n                # Log first 10 samples\n                if len(sample_predictions) < 10:\n                    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n                    axes[0].imshow(pred)\n                    axes[0].set_title('Prediction')\n                    axes[0].axis('off')\n                    axes[1].imshow(target)\n                    axes[1].set_title('Ground Truth')\n                    axes[1].axis('off')\n                    plt.tight_layout()\n                    sample_predictions.append(wandb.Image(fig))\n                    plt.close()\n                \n                pred_ball = detector.detect_ball(pred)\n                target_ball = detector.detect_ball(target)\n                \n                if pred_ball is None or target_ball is None:\n                    failed_predictions += 1\n                    continue\n                \n                pos_error_x = abs(pred_ball['position'][0] - target_ball['position'][0])\n                pos_error_y = abs(pred_ball['position'][1] - target_ball['position'][1])\n                \n                all_errors_x.append(pos_error_x)\n                all_errors_y.append(pos_error_y)\n    \n    failure_rate = (failed_predictions / total_predictions) * 100\n    \n    results = {\n        'test/position_x_mean': np.mean(all_errors_x) if len(all_errors_x) > 0 else 0,\n        'test/position_x_std': np.std(all_errors_x) if len(all_errors_x) > 0 else 0,\n        'test/position_y_mean': np.mean(all_errors_y) if len(all_errors_y) > 0 else 0,\n        'test/position_y_std': np.std(all_errors_y) if len(all_errors_y) > 0 else 0,\n        'test/failure_rate': failure_rate,\n        'test/total_predictions': total_predictions,\n        'test/failed_predictions': failed_predictions\n    }\n    \n    wandb.log(results)\n    wandb.log({\"test/sample_predictions\": sample_predictions})\n    \n    comparison_table = wandb.Table(\n        columns=[\"Metric\", \"Paper (Pix2Pix)\", \"Our Results\"],\n        data=[\n            [\"Position X Error (pixels)\", \"6.28 ± 7.98\", f\"{results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f}\"],\n            [\"Position Y Error (pixels)\", \"11.7 ± 12.8\", f\"{results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f}\"],\n            [\"Failure Rate (%)\", \"7%\", f\"{failure_rate:.2f}%\"]\n        ]\n    )\n    wandb.log({\"comparison_with_paper\": comparison_table})\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EVALUATION RESULTS\")\n    print(f\"Total: {total_predictions}, Failed: {failed_predictions} ({failure_rate:.2f}%)\")\n    \n    if len(all_errors_x) > 0:\n        print(f\"\\nPosition X Error: {results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f} pixels\")\n        print(f\"Position Y Error: {results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f} pixels\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"COMPARISON WITH PAPER RESULTS\")\n    print(\"Paper Pix2Pix Results:\")\n    print(\"  Position X: 6.28 ± 7.98 pixels\")\n    print(\"  Position Y: 11.7 ± 12.8 pixels\")\n    print(\"  Rotation: 17.2 ± 20.8 degrees\")\n    print(\"  Roundness: 0.56 ± 0.14 pixels\")\n    print(\"  Failure Rate: 7%\")\n    print(\"  Number of balls: 93% single ball, 7% no/multiple balls\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.862680Z","iopub.execute_input":"2025-12-02T16:21:51.862961Z","iopub.status.idle":"2025-12-02T16:21:51.880362Z","shell.execute_reply.started":"2025-12-02T16:21:51.862938Z","shell.execute_reply":"2025-12-02T16:21:51.879723Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def main(use_wandb=True, wandb_project=\"physicsgen-pix2pix\", wandb_entity=None):\n    DATA_ROOT = '/kaggle/input/bounce-ball/bounce_ball'\n    \n    CONFIG = {\n        'batch_size': 18,\n        'lr_discriminator': 1e-4, #try changing to 5e-5\n        'lr_generator': 2e-4,\n        'num_epochs': 50,\n        'lambda_l1': 100,\n        'lambda_gp': 10,\n        'image_size': 256,\n        'num_workers': 2,\n        'split_direction': 'horizontal',\n    }\n    \n    if use_wandb:\n        wandb.init(\n            project=wandb_project,\n            entity=wandb_entity,\n            config=CONFIG,\n            name=f\"pix2pix-instancenorm-gp\",\n            tags=['pix2pix', 'instancenorm', 'gradient-penalty', 'physicsgen']\n        )\n        config = wandb.config\n    else:\n        config = CONFIG\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    train_loader, val_loader, test_loader = create_data_loaders(\n        DATA_ROOT,\n        batch_size=config['batch_size'],\n        image_size=config['image_size'],\n        num_workers=config['num_workers'],\n        split_direction=config['split_direction']\n    )\n    \n    generator = Generator().to(device)\n    discriminator = Discriminator().to(device)\n    print(f\"\\nGenerator params: {sum(p.numel() for p in generator.parameters()):,}\")\n    print(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n\n    if use_wandb:\n        wandb.watch(generator, log='all', log_freq=100)\n        wandb.watch(discriminator, log='all', log_freq=100)\n    \n    train_pix2pix(generator, discriminator, train_loader, val_loader, config, device)\n    \n    if use_wandb:\n        wandb.finish()\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.881873Z","iopub.execute_input":"2025-12-02T16:21:51.882501Z","iopub.status.idle":"2025-12-02T16:21:51.897471Z","shell.execute_reply.started":"2025-12-02T16:21:51.882482Z","shell.execute_reply":"2025-12-02T16:21:51.896668Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == '__main__':\n    DATA_ROOT = '/kaggle/input/bounce-ball/bounce_ball'\n    USE_WANDB = True\n    WANDB_PROJECT = \"physicsgen-pix2pix\"\n    \n    generator, discriminator = main(\n        use_wandb=USE_WANDB,\n        wandb_project=WANDB_PROJECT\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:21:51.898206Z","iopub.execute_input":"2025-12-02T16:21:51.898449Z","execution_failed":"2025-12-02T21:02:00.748Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251202_162151-lgd90wz4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix/runs/lgd90wz4' target=\"_blank\">pix2pix-instancenorm-gp</a></strong> to <a href='https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix' target=\"_blank\">https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix/runs/lgd90wz4' target=\"_blank\">https://wandb.ai/dhritik-carnegie-mellon-university/physicsgen-pix2pix/runs/lgd90wz4</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nSplit direction: horizontal\n\nTrain set:\n44835 combined images from /kaggle/input/bounce-ball/bounce_ball/train_double\n  Image size: 1024x512\n  Detected horizontal split (width > height)\n  Will split into: 512x512 (left) and 512x512 (right)\n\nValidation set:\n58 combined images from /kaggle/input/bounce-ball/bounce_ball/val_double\n  Image size: 1024x512\n  Detected horizontal split (width > height)\n  Will split into: 512x512 (left) and 512x512 (right)\n\nTest set:\n1600 combined images from /kaggle/input/bounce-ball/bounce_ball/test_double\n  Image size: 1024x512\n  Detected horizontal split (width > height)\n  Will split into: 512x512 (left) and 512x512 (right)\n\nGenerator params: 54,416,003\nDiscriminator params: 2,771,648\nStarting Pix2Pix Training with Gradient Penalty\n======================================================================\nLambda L1: 100\nLambda GP: 10\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45646e1feda44a8897157e0e286c3943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94440a739f06427fa44db991bc789c5b"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [1/50] | Time: 1125.3s\n  Train - G: 15.5230, D: 0.5367, GP: 0.3276\n  Val   - G: 15.0174, D: 0.1539\n  ✓ Saved best model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1e19b43ae64d179a3eb9bb6b41d604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d8d5b24eba43db84ad4b26323c8314"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [2/50] | Time: 1122.5s\n  Train - G: 15.1867, D: 0.1467, GP: 0.0153\n  Val   - G: 15.1047, D: 0.1190\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1dc322307504bc2acba5f3a1836d8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a29c2272ee48598ed47fdd302331b1"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [3/50] | Time: 1122.5s\n  Train - G: 15.1894, D: 0.1243, GP: 0.0156\n  Val   - G: 15.1567, D: 0.1038\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c44e6f419fa44d38839a595efe50e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10abf98f8cbf4c6a88ac675d57ee8dad"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [4/50] | Time: 1122.6s\n  Train - G: 15.1733, D: 0.1310, GP: 0.0215\n  Val   - G: 15.1090, D: 0.1010\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f710fdc3f2334f6e9e457f0c1d6fcb9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa462e3cb9c49f2bd27a4e19b05a985"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [5/50] | Time: 1122.2s\n  Train - G: 15.1796, D: 0.1149, GP: 0.0148\n  Val   - G: 15.1499, D: 0.0963\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee54e578b4b4437869540bbb7966d4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bdafc5f93b945f4bb3dd26a0e16d06e"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [6/50] | Time: 1124.0s\n  Train - G: 15.1851, D: 0.1074, GP: 0.0122\n  Val   - G: 15.1404, D: 0.0948\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6368c8e74b384a049ea461e5a72260bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1ebaa0a9d24197aa6f2b4f0b228cdc"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [7/50] | Time: 1123.1s\n  Train - G: 15.1783, D: 0.1096, GP: 0.0132\n  Val   - G: 15.2285, D: 0.0900\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934006a216cd476c800a4a48d75bc83e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ef04b29c8748f7984ef695a79eabcb"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [8/50] | Time: 1122.3s\n  Train - G: 15.1781, D: 0.1048, GP: 0.0112\n  Val   - G: 15.2383, D: 0.0904\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130c8ebe7dbc4899b3642f803a1c6aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a11584a00f48468c0c8e7a5f88ce98"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [9/50] | Time: 1123.7s\n  Train - G: 15.1772, D: 0.0976, GP: 0.0080\n  Val   - G: 15.2997, D: 0.0921\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f131565a16047b1832c1ce4aca9c521"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd347adb275a4f479dc33dbae2cd7eef"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [10/50] | Time: 1122.5s\n  Train - G: 15.1630, D: 0.1065, GP: 0.0119\n  Val   - G: 15.2015, D: 0.0885\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba5c54844774f419fdfbb299ddeada0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 11 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ad031379674657a9c3ce7cd072f2dc"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [11/50] | Time: 1119.7s\n  Train - G: 15.1615, D: 0.0976, GP: 0.0077\n  Val   - G: 15.3039, D: 0.0915\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ab79ee32dc4fc4943b1c8340cf4805"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 12 - Validation:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb057305a634c73aec6e7a94120346f"}},"metadata":{}},{"name":"stdout","text":"\nEpoch [12/50] | Time: 1121.4s\n  Train - G: 15.1566, D: 0.0961, GP: 0.0073\n  Val   - G: 15.2009, D: 0.0843\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13 - Training:   0%|          | 0/2491 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448cc2261d2b497288db59086a1ed8ad"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}