{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13480612,"sourceType":"datasetVersion","datasetId":8558578}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport time\nimport json\nfrom pathlib import Path\n\ntry:\n    import wandb\nexcept ImportError:\n    !pip install wandb -q\n    import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:44:02.469329Z","iopub.execute_input":"2025-10-24T03:44:02.469532Z","iopub.status.idle":"2025-10-24T03:44:02.482878Z","shell.execute_reply.started":"2025-10-24T03:44:02.469509Z","shell.execute_reply":"2025-10-24T03:44:02.482395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['WANDB_API_KEY'] = 'c1a8356072f55deda375cb8d821628d3b6962f9a'\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:44:02.498054Z","iopub.execute_input":"2025-10-24T03:44:02.498252Z","iopub.status.idle":"2025-10-24T03:44:02.512135Z","shell.execute_reply.started":"2025-10-24T03:44:02.498238Z","shell.execute_reply":"2025-10-24T03:44:02.511420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self, latent_dim=128, input_channels=3, image_size=256):\n        super(Encoder, self).__init__()\n        \n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(256)\n        \n        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n        self.bn5 = nn.BatchNorm2d(512)\n        \n        final_size = image_size // (2**5)\n        self.flatten_size = 512 * final_size * final_size\n        \n        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n        self.fc_log_var = nn.Linear(self.flatten_size, latent_dim)\n        \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = F.relu(self.bn5(self.conv5(x)))\n        \n        x = x.view(x.size(0), -1)\n        \n        mu = self.fc_mu(x)\n        log_var = self.fc_log_var(x)\n        \n        return mu, log_var\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:44:02.525421Z","iopub.execute_input":"2025-10-24T03:44:02.526048Z","iopub.status.idle":"2025-10-24T03:44:02.537382Z","shell.execute_reply.started":"2025-10-24T03:44:02.526026Z","shell.execute_reply":"2025-10-24T03:44:02.536705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self, latent_dim=128, output_channels=3, image_size=256):\n        super(Decoder, self).__init__()\n        \n        self.init_size = image_size // (2**5)\n        self.init_channels = 512\n        \n        self.fc = nn.Linear(latent_dim, self.init_channels * self.init_size * self.init_size)\n        \n        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(256)\n        \n        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(64)\n        \n        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(32)\n        \n        self.deconv5 = nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1)\n        \n    def forward(self, z):\n        x = self.fc(z)\n        x = x.view(x.size(0), self.init_channels, self.init_size, self.init_size)\n        \n        x = F.relu(self.bn1(self.deconv1(x)))\n        x = F.relu(self.bn2(self.deconv2(x)))\n        x = F.relu(self.bn3(self.deconv3(x)))\n        x = F.relu(self.bn4(self.deconv4(x)))\n        x = torch.sigmoid(self.deconv5(x))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:44:02.551828Z","iopub.execute_input":"2025-10-24T03:44:02.552066Z","iopub.status.idle":"2025-10-24T03:44:02.565426Z","shell.execute_reply.started":"2025-10-24T03:44:02.552047Z","shell.execute_reply":"2025-10-24T03:44:02.564932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VAE(nn.Module):\n    \n    def __init__(self, latent_dim=128, input_channels=3, image_size=256):\n        super(VAE, self).__init__()\n        \n        self.latent_dim = latent_dim\n        self.encoder = Encoder(latent_dim, input_channels, image_size)\n        self.decoder = Decoder(latent_dim, input_channels, image_size)\n        \n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def forward(self, x):\n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        reconstruction = self.decoder(z)\n        return reconstruction, mu, log_var\n\n\ndef vae_loss(reconstruction, target, mu, log_var, beta=1.0):\n    \n    recon_loss = F.mse_loss(reconstruction, target, reduction='sum')\n    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    total_loss = recon_loss + beta * kl_loss\n    return total_loss, recon_loss, kl_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:44:02.582332Z","iopub.execute_input":"2025-10-24T03:44:02.582480Z","iopub.status.idle":"2025-10-24T03:44:02.593188Z","shell.execute_reply.started":"2025-10-24T03:44:02.582468Z","shell.execute_reply":"2025-10-24T03:44:02.592601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MotionDataset(Dataset):\n    def __init__(self, split_dir, transform=None, task=\"autoencoder\"):\n        self.dir = split_dir\n        self.transform = transform\n        self.task = task\n        self.files = sorted(f for f in os.listdir(self.dir)\n                            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\")))\n        assert self.files, f\"No images in {self.dir}\"\n\n    def __getitem__(self, i):\n        path = os.path.join(self.dir, self.files[i])\n        img = Image.open(path).convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n\n        if self.task == \"autoencoder\":\n            inp = tgt = img\n        elif self.task == \"denoise\":\n            noisy = img + 0.05*torch.randn_like(img)   # example\n            noisy = torch.clamp(noisy, 0, 1)\n            inp, tgt = noisy, img\n        else:\n            # extend for other tasks (masking, SR, etc.)\n            inp, tgt = img, img\n\n        return inp, tgt\n\n    def __len__(self): return len(self.files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:57:47.304402Z","iopub.execute_input":"2025-10-24T03:57:47.305002Z","iopub.status.idle":"2025-10-24T03:57:47.311925Z","shell.execute_reply.started":"2025-10-24T03:57:47.304982Z","shell.execute_reply":"2025-10-24T03:57:47.311138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loaders(data_root, batch_size=32, image_size=256, num_workers=2):\n    \n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n    ])\n    \n    \n    print(\"\\nTrain set:\")\n    #train_dataset = MotionDataset(os.path.join(data_root, 'train/input'),os.path.join(data_root, 'train/target'),transform=transform)\n    train_dataset = MotionDataset(os.path.join(data_root, 'train'),transform=transform)\n    \n    print(\"\\nValidation set:\")\n    val_dataset = MotionDataset(os.path.join(data_root, 'val'), transform=transform)\n    \n    print(\"\\nTest set:\")\n    test_dataset = MotionDataset(os.path.join(data_root, 'test'), transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n                             shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n                           shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n                            shuffle=False, num_workers=num_workers)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:53:56.008593Z","iopub.execute_input":"2025-10-24T03:53:56.009151Z","iopub.status.idle":"2025-10-24T03:53:56.015761Z","shell.execute_reply.started":"2025-10-24T03:53:56.009128Z","shell.execute_reply":"2025-10-24T03:53:56.015135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, device, beta=1.0, epoch=0):\n    model.train()\n    epoch_loss = 0\n    epoch_recon_loss = 0\n    epoch_kl_loss = 0\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} - Training')\n    for batch_idx, batch in enumerate(pbar):\n        if isinstance(batch, (list, tuple)):\n            if len(batch) >= 2:\n                input_img, target_img = batch[0], batch[1]\n            else:\n                input_img = target_img = batch[0]\n        elif isinstance(batch, dict):\n            input_img  = batch.get('input', next(iter(batch.values())))\n            target_img = batch.get('target', input_img)\n        else:\n            input_img = target_img = batch\n\n        input_img  = input_img.to(device, non_blocking=True)\n        target_img = target_img.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n        reconstruction, mu, log_var = model(input_img)\n        loss, recon_loss, kl_loss = vae_loss(reconstruction, target_img, mu, log_var, beta)\n\n        loss.backward()\n        optimizer.step()\n\n        batch_size = len(input_img)\n        epoch_loss       += loss.item()\n        epoch_recon_loss += recon_loss.item()\n        epoch_kl_loss    += kl_loss.item()\n\n        if batch_idx % 10 == 0:\n            wandb.log({\n                'batch/loss': loss.item() / batch_size,\n                'batch/reconstruction_loss': recon_loss.item() / batch_size,\n                'batch/kl_loss': kl_loss.item() / batch_size,\n                'batch/step': epoch * len(train_loader) + batch_idx\n            })\n\n        pbar.set_postfix({\n            'loss':  f'{loss.item()/batch_size:.4f}',\n            'recon': f'{recon_loss.item()/batch_size:.4f}',\n            'kl':    f'{kl_loss.item()/batch_size:.4f}'\n        })\n\n    num_samples = len(train_loader.dataset)\n    return epoch_loss/num_samples, epoch_recon_loss/num_samples, epoch_kl_loss/num_samples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:02:34.222821Z","iopub.execute_input":"2025-10-24T04:02:34.223473Z","iopub.status.idle":"2025-10-24T04:02:34.232019Z","shell.execute_reply.started":"2025-10-24T04:02:34.223446Z","shell.execute_reply":"2025-10-24T04:02:34.231141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(model, val_loader, device, beta=1.0, epoch=0):\n    model.eval()\n    val_loss = 0\n    val_recon_loss = 0\n    val_kl_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} - Validation'):\n            if isinstance(batch, (list, tuple)):\n                if len(batch) >= 2:\n                    input_img, target_img = batch[0], batch[1]\n                else:\n                    input_img = target_img = batch[0]\n            elif isinstance(batch, dict):\n                input_img  = batch.get('input', next(iter(batch.values())))\n                target_img = batch.get('target', input_img)\n            else:\n                input_img = target_img = batch\n\n            input_img  = input_img.to(device, non_blocking=True)\n            target_img = target_img.to(device, non_blocking=True)\n\n            reconstruction, mu, log_var = model(input_img)\n            loss, recon_loss, kl_loss = vae_loss(reconstruction, target_img, mu, log_var, beta)\n\n            val_loss       += loss.item()\n            val_recon_loss += recon_loss.item()\n            val_kl_loss    += kl_loss.item()\n\n    num_samples = len(val_loader.dataset)\n    return val_loss/num_samples, val_recon_loss/num_samples, val_kl_loss/num_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:02.236261Z","iopub.execute_input":"2025-10-24T03:54:02.236996Z","iopub.status.idle":"2025-10-24T03:54:02.242839Z","shell.execute_reply.started":"2025-10-24T03:54:02.236970Z","shell.execute_reply":"2025-10-24T03:54:02.242016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_results(model, val_loader, device, epoch=0):\n    model.eval()\n    \n    batch = next(iter(val_loader))\n    if isinstance(batch, (list, tuple)):\n        if len(batch) >= 2:\n            input_batch, target_batch = batch[0], batch[1]\n        else:\n            input_batch = target_batch = batch[0]\n    elif isinstance(batch, dict):\n        input_batch  = batch.get(\"input\", next(iter(batch.values())))\n        target_batch = batch.get(\"target\", input_batch)\n    else:\n        input_batch = target_batch = batch\n\n    with torch.no_grad():\n        input_batch  = input_batch.to(device, non_blocking=True)\n        target_batch = target_batch.to(device, non_blocking=True)\n\n        recon_batch, _, _ = model(input_batch)\n        num_vis = min(4, input_batch.shape[0])\n        fig, axes = plt.subplots(3, num_vis, figsize=(4 * num_vis, 12))\n        \n        def to_img(t):\n            x = t.detach().float().cpu()\n            x = (x * 0.5) + 0.5\n            x = x.clamp(0, 1)\n            return x\n\n        for i in range(num_vis):\n            inp  = to_img(input_batch[i]).permute(1, 2, 0).numpy()\n            rec  = to_img(recon_batch[i]).permute(1, 2, 0).numpy()\n            targ = to_img(target_batch[i]).permute(1, 2, 0).numpy()\n\n            axes[0, i].imshow(inp);  axes[0, i].set_title(f'Input {i+1}');          axes[0, i].axis('off')\n            axes[1, i].imshow(rec);  axes[1, i].set_title(f'Reconstruction {i+1}'); axes[1, i].axis('off')\n            axes[2, i].imshow(targ); axes[2, i].set_title(f'Ground Truth {i+1}');   axes[2, i].axis('off')\n\n        plt.tight_layout()\n        wandb.log({f\"reconstructions/epoch_{epoch+1}\": wandb.Image(fig), \"epoch\": epoch + 1})\n        plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:04.631977Z","iopub.execute_input":"2025-10-24T03:54:04.632662Z","iopub.status.idle":"2025-10-24T03:54:04.639968Z","shell.execute_reply.started":"2025-10-24T03:54:04.632636Z","shell.execute_reply":"2025-10-24T03:54:04.639228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, device, num_epochs=100, \n                learning_rate=1e-3, beta=1.0, save_dir='checkpoints'):\n    \n    os.makedirs(save_dir, exist_ok=True)\n    \n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n    \n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    print(\"Starting Training\")\n    print(f\"Device: {device}\")\n    print(f\"Total epochs: {num_epochs}\")\n    print(f\"Train samples: {len(train_loader.dataset)}\")\n    print(f\"Val samples: {len(val_loader.dataset)}\")\n    print(f\"Batch size: {train_loader.batch_size}\")\n    print(f\"W&B Project: {wandb.run.project}\")\n    print(f\"W&B Run: {wandb.run.name}\")\n    \n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        \n        \n        train_loss, train_recon, train_kl = train_epoch(\n            model, train_loader, optimizer, device, beta, epoch\n        )\n        \n        val_loss, val_recon, val_kl = validate(model, val_loader, device, beta, epoch)\n        old_lr = optimizer.param_groups[0]['lr']\n        scheduler.step(val_loss)\n        new_lr = optimizer.param_groups[0]['lr']\n        \n        train_losses.append((train_loss, train_recon, train_kl))\n        val_losses.append((val_loss, val_recon, val_kl))\n        epoch_time = time.time() - epoch_start\n        \n        wandb.log({\n            'epoch': epoch + 1,\n            'train/loss': train_loss,\n            'train/reconstruction_loss': train_recon,\n            'train/kl_loss': train_kl,\n            'val/loss': val_loss,\n            'val/reconstruction_loss': val_recon,\n            'val/kl_loss': val_kl,\n            'learning_rate': new_lr,\n            'epoch_time': epoch_time\n        })\n        \n        print(f\"\\nEpoch [{epoch+1:3d}/{num_epochs}] | Time: {epoch_time:.1f}s | LR: {new_lr:.2e}\")\n        print(f\"  Train Loss: {train_loss:.6f} (Recon: {train_recon:.6f}, KL: {train_kl:.6f})\")\n        print(f\"  Val   Loss: {val_loss:.6f} (Recon: {val_recon:.6f}, KL: {val_kl:.6f})\")\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint_path = os.path.join(save_dir, 'best_vae_model.pth')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, checkpoint_path)\n            \n            wandb.save(checkpoint_path)\n            \n            print(f\"  ✓ Saved best model (val_loss: {val_loss:.6f})\")\n            \n            artifact = wandb.Artifact('vae-model', type='model')\n            artifact.add_file(checkpoint_path)\n            wandb.log_artifact(artifact)\n        \n        if (epoch + 1) % 10 == 0:\n            visualize_results(model, val_loader, device, epoch)\n        \n        if (epoch + 1) % 20 == 0:\n            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, checkpoint_path)\n    \n    total_time = time.time() - start_time\n    \n    wandb.run.summary['best_val_loss'] = best_val_loss\n    wandb.run.summary['total_training_time'] = total_time\n    wandb.run.summary['total_epochs'] = num_epochs\n    \n    print(f\"Training completed in {total_time/3600:.2f} hours\")\n    print(f\"Best validation loss: {best_val_loss:.6f}\")\n    plot_losses(train_losses, val_losses)\n    \n    return train_losses, val_losses\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:07.359091Z","iopub.execute_input":"2025-10-24T03:54:07.359599Z","iopub.status.idle":"2025-10-24T03:54:07.372398Z","shell.execute_reply.started":"2025-10-24T03:54:07.359574Z","shell.execute_reply":"2025-10-24T03:54:07.371831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_losses(train_losses, val_losses):\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    epochs = range(1, len(train_losses) + 1)\n    \n    axes[0].plot(epochs, [l[0] for l in train_losses], label='Train', linewidth=2)\n    axes[0].plot(epochs, [l[0] for l in val_losses], label='Val', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Total Loss')\n    axes[0].set_title('Total Loss (Reconstruction + KL)')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].plot(epochs, [l[1] for l in train_losses], label='Recon (Train)', linewidth=2)\n    axes[1].plot(epochs, [l[2] for l in train_losses], label='KL (Train)', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Loss')\n    axes[1].set_title('Reconstruction vs KL Divergence')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    wandb.log({\"loss_curves\": wandb.Image(fig)})\n    \n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:11.590860Z","iopub.execute_input":"2025-10-24T03:54:11.591598Z","iopub.status.idle":"2025-10-24T03:54:11.598691Z","shell.execute_reply.started":"2025-10-24T03:54:11.591571Z","shell.execute_reply":"2025-10-24T03:54:11.597916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BallDetector:\n    \n    def __init__(self, ball_radius=15, min_radius=10, max_radius=25):\n        self.ball_radius = ball_radius\n        self.min_radius = min_radius\n        self.max_radius = max_radius\n    \n    def detect_ball(self, image):\n        \n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n        \n        blurred = cv2.GaussianBlur(gray, (9, 9), 2)\n        \n        circles = cv2.HoughCircles(\n            blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=50,\n            param1=50, param2=30,\n            minRadius=self.min_radius, maxRadius=self.max_radius\n        )\n        \n        if circles is None or len(circles[0]) == 0:\n            return None\n        \n        circle = circles[0][0]\n        center_x, center_y, radius = circle\n        \n        return {\n            'position': (center_x, center_y),\n            'radius': radius\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:13.598265Z","iopub.execute_input":"2025-10-24T03:54:13.598554Z","iopub.status.idle":"2025-10-24T03:54:13.605407Z","shell.execute_reply.started":"2025-10-24T03:54:13.598532Z","shell.execute_reply":"2025-10-24T03:54:13.604671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_loader, device):\n    \n    model.eval()\n    detector = BallDetector()\n\n    all_errors_x = []\n    all_errors_y = []\n    failed_predictions = 0\n    total_predictions = 0\n\n    sample_predictions = []\n    print(\"Evaluating Model\")\n    \n    def denorm01(t):\n        x = (t * 0.5) + 0.5\n        return x.clamp(0, 1)\n\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(tqdm(test_loader, desc='Evaluating')):\n            if isinstance(batch, (list, tuple)):\n                if len(batch) >= 2:\n                    input_img, target_img = batch[0], batch[1]\n                else:\n                    input_img = target_img = batch[0]\n            elif isinstance(batch, dict):\n                input_img  = batch.get('input', next(iter(batch.values())))\n                target_img = batch.get('target', input_img)\n            else:\n                input_img = target_img = batch\n                \n            input_img  = input_img.to(device, non_blocking=True)\n            target_img = target_img.to(device, non_blocking=True)\n\n            reconstruction, _, _ = model(input_img)\n            pred_imgs   = denorm01(reconstruction).cpu().numpy()\n            target_imgs = denorm01(target_img).cpu().numpy()\n\n            for i in range(len(pred_imgs)):\n                total_predictions += 1\n                pred   = (pred_imgs[i].transpose(1, 2, 0) * 255.0).round().astype(np.uint8)\n                target = (target_imgs[i].transpose(1, 2, 0) * 255.0).round().astype(np.uint8)\n                \n                if len(sample_predictions) < 10:\n                    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n                    axes[0].imshow(pred)\n                    axes[0].set_title('Prediction')\n                    axes[0].axis('off')\n                    axes[1].imshow(target)\n                    axes[1].set_title('Ground Truth')\n                    axes[1].axis('off')\n                    plt.tight_layout()\n                    sample_predictions.append(wandb.Image(fig))\n                    plt.close()\n\n                \n                pred_ball   = detector.detect_ball(pred)\n                target_ball = detector.detect_ball(target)\n\n                if pred_ball is None or target_ball is None:\n                    failed_predictions += 1\n                    continue\n                    \n                pos_error_x = abs(pred_ball['position'][0] - target_ball['position'][0])\n                pos_error_y = abs(pred_ball['position'][1] - target_ball['position'][1])\n\n                all_errors_x.append(pos_error_x)\n                all_errors_y.append(pos_error_y)\n\n    \n    if total_predictions == 0:\n        failure_rate = 0.0\n    else:\n        failure_rate = (failed_predictions / total_predictions) * 100.0\n\n    results = {\n        'test/position_x_mean': float(np.mean(all_errors_x)) if all_errors_x else 0.0,\n        'test/position_x_std':  float(np.std(all_errors_x))  if all_errors_x else 0.0,\n        'test/position_y_mean': float(np.mean(all_errors_y)) if all_errors_y else 0.0,\n        'test/position_y_std':  float(np.std(all_errors_y))  if all_errors_y else 0.0,\n        'test/failure_rate':    failure_rate,\n        'test/total_predictions': total_predictions,\n        'test/failed_predictions': failed_predictions,\n    }\n    \n    try:\n        wandb.log(results)\n        if sample_predictions:\n            wandb.log({\"test/sample_predictions\": sample_predictions})\n        comparison_table = wandb.Table(\n            columns=[\"Metric\", \"Paper (VAE)\", \"Our Results\"],\n            data=[\n                [\"Position X Error (pixels)\", \"4.69 ± 6.1\", f\"{results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f}\"],\n                [\"Position Y Error (pixels)\", \"6.25 ± 6.9\", f\"{results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f}\"],\n                [\"Failure Rate (%)\", \"95%\", f\"{failure_rate:.2f}%\"]\n            ]\n        )\n        wandb.log({\"comparison_with_paper\": comparison_table})\n    except Exception as e:\n        print(f\"(wandb logging skipped: {e})\")\n        \n    print(\"EVALUATION RESULTS\")\n    print(f\"\\nTotal predictions: {total_predictions}\")\n    print(f\"Failed predictions: {failed_predictions} ({failure_rate:.2f}%)\")\n    if all_errors_x:\n        print(f\"\\nPosition X Error: {results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f} pixels\")\n        print(f\"Position Y Error: {results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f} pixels\")\n        \n    print(\"COMPARISON WITH PAPER RESULTS (Table 3)\")\n    print(\"Paper VAE Results:\")\n    print(\"  Position X: 4.69 ± 6.1 pixels\")\n    print(\"  Position Y: 6.25 ± 6.9 pixels\")\n    print(\"  Failure Rate: 95%\")\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:15.740074Z","iopub.execute_input":"2025-10-24T03:54:15.740577Z","iopub.status.idle":"2025-10-24T03:54:15.754023Z","shell.execute_reply.started":"2025-10-24T03:54:15.740555Z","shell.execute_reply":"2025-10-24T03:54:15.753177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(use_wandb=True, wandb_project=\"physicsgen-vae\", wandb_entity=None):\n    DATA_ROOT = '/kaggle/input/ball-data/ball_bounce/ball_bounce' \n    CONFIG = {\n        'latent_dim': 128,\n        'image_size': 256,\n        'batch_size': 32,\n        'num_epochs': 10,\n        'learning_rate': 1e-3,\n        'beta': 1.0,\n        'num_workers': 2,\n        'motion_type': 'bouncing',\n    }\n    \n    if use_wandb:\n        wandb.init(\n            project=wandb_project,\n            entity=wandb_entity,\n            config=CONFIG,\n            name=f\"vae-{CONFIG['motion_type']}-latent{CONFIG['latent_dim']}\",\n            tags=['vae', 'physicsgen', CONFIG['motion_type']],\n            notes=\"Replicating VAE baseline from PhysicsGen CVPR 2025\"\n        )\n        config = wandb.config\n    else:\n        config = CONFIG\n    \n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    if device.type == 'cuda':\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        if use_wandb:\n            wandb.config.update({\n                'gpu': torch.cuda.get_device_name(0),\n                'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9\n            })\n    \n    train_loader, val_loader, test_loader = create_data_loaders(\n        DATA_ROOT,\n        batch_size=config['batch_size'],\n        image_size=config['image_size'],\n        num_workers=config['num_workers']\n    )\n    \n    print(\"Initializing Model\")\n    model = VAE(\n        latent_dim=config['latent_dim'],\n        input_channels=3,\n        image_size=config['image_size']\n    ).to(device)\n    \n    num_params = sum(p.numel() for p in model.parameters())\n    print(f\"Model parameters: {num_params:,}\")\n    \n    if use_wandb:\n        wandb.config.update({'model_parameters': num_params})\n        wandb.watch(model, log='all', log_freq=100)\n    \n    train_losses, val_losses = train_model(\n        model, train_loader, val_loader, device,\n        num_epochs=config['num_epochs'],\n        learning_rate=config['learning_rate'],\n        beta=config['beta']\n    )\n    \n    \n    results = evaluate_model(model, test_loader, device)\n    if use_wandb:\n        wandb.finish()\n    \n    print(\"\\n✓ Training and evaluation complete!\")\n    print(\"✓ Check W&B dashboard for detailed logs and visualizations\")\n    \n    return model, results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:54:18.918902Z","iopub.execute_input":"2025-10-24T03:54:18.919486Z","iopub.status.idle":"2025-10-24T03:54:18.929806Z","shell.execute_reply.started":"2025-10-24T03:54:18.919465Z","shell.execute_reply":"2025-10-24T03:54:18.929054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    DATA_ROOT = '/kaggle/input/ball-data/ball_bounce/ball_bounce'\n    WANDB_PROJECT = \"physicsgen-vae-replication\"  # Your project name\n    WANDB_ENTITY = None\n    USE_WANDB = True \n    CONFIG_TEST = {\n        'latent_dim': 128,\n        'image_size': 256,\n        'batch_size': 32,\n        'num_epochs': 10,\n        'learning_rate': 1e-3,\n        'beta': 1.0,\n        'num_workers': 2,\n        'motion_type': 'bouncing',\n    }\n    \n    print(\"PhysicsGen VAE Baseline - Kaggle Training Script\")\n    print(f\"W&B Logging: {'Enabled' if USE_WANDB else 'Disabled'}\")\n    print(f\"Data Path: {DATA_ROOT}\")\n    \n    model, results = main(\n        use_wandb=USE_WANDB,\n        wandb_project=WANDB_PROJECT,\n        wandb_entity=WANDB_ENTITY\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:02:45.261631Z","iopub.execute_input":"2025-10-24T04:02:45.261906Z","iopub.status.idle":"2025-10-24T04:03:30.898716Z","shell.execute_reply.started":"2025-10-24T04:02:45.261887Z","shell.execute_reply":"2025-10-24T04:03:30.897570Z"}},"outputs":[],"execution_count":null}]}