{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13480612,"sourceType":"datasetVersion","datasetId":8558578}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nPix2Pix Implementation for PhysicsGen Motion Prediction\nBased on paper appendix - Table 5 hyperparameters\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport time\nfrom pathlib import Path\nimport itertools\n\ntry:\n    import wandb\nexcept ImportError:\n    print(\"Installing wandb...\")\n    !pip install wandb -q\n    import wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ['WANDB_API_KEY'] = 'c1a8356072f55deda375cb8d821628d3b6962f9a'\nwandb.login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CombinedImageDataset(Dataset):\n    def __init__(self, data_dir, transform=None, split_direction='horizontal'):\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.split_direction = split_direction\n        self.image_files = sorted([\n            f for f in os.listdir(self.data_dir) \n            if f.endswith(('.png', '.jpg', '.jpeg'))\n        ])\n        \n        print(f\"{len(self.image_files)} combined images from {data_dir}\")\n        \n        if self.image_files:\n            first_img = Image.open(self.data_dir / self.image_files[0])\n            w, h = first_img.size\n            print(f\"  Image size: {w}x{h}\")\n            \n            if split_direction == 'horizontal':\n                if w > h * 1.5:  # Likely side-by-side\n                    print(f\"  ✓ Detected horizontal split (width > height)\")\n                    print(f\"  Will split into: {w//2}x{h} (left) and {w//2}x{h} (right)\")\n                else:\n                    print(f\"  Image might not be horizontally combined\")\n            else:\n                if h > w * 1.5:  # Likely stacked vertically\n                    print(f\"  ✓ Detected vertical split (height > width)\")\n                    print(f\"  Will split into: {w}x{h//2} (top) and {w}x{h//2} (bottom)\")\n                else:\n                    print(f\"  Image might not be vertically combined\")\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_path = self.data_dir / self.image_files[idx]\n        combined_img = Image.open(img_path).convert('RGB')\n        \n        w, h = combined_img.size\n        \n        if self.split_direction == 'horizontal':\n            # Split left and right\n            mid = w // 2\n            input_img = combined_img.crop((0, 0, mid, h))\n            target_img = combined_img.crop((mid, 0, w, h))\n        else:\n            # Split top and bottom\n            mid = h // 2\n            input_img = combined_img.crop((0, 0, w, mid))\n            target_img = combined_img.crop((0, mid, w, h))\n        \n        if self.transform:\n            input_img = self.transform(input_img)\n            target_img = self.transform(target_img)\n        \n        return input_img, target_img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loaders(data_root, batch_size=18, image_size=256, num_workers=2, \n                       split_direction='horizontal'):\n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    \n    print(f\"Split direction: {split_direction}\")\n    \n    \n    train_path = Path(data_root) / 'train'\n    print(\"\\nTrain set:\")\n    train_dataset = CombinedImageDataset(\n        str(train_path), \n        transform=transform,\n        split_direction=split_direction\n    )\n    \n    val_path = Path(data_root) / 'val'\n    test_path = Path(data_root) / 'test'\n    \n    if val_path.exists():\n        print(\"\\nValidation set:\")\n        val_dataset = CombinedImageDataset(\n            str(val_path), \n            transform=transform,\n            split_direction=split_direction\n        )\n    else:\n        print(\"\\nValidation set: Creating 10% split from training\")\n        train_size = int(0.9 * len(train_dataset))\n        val_size = len(train_dataset) - train_size\n        train_dataset, val_dataset = torch.utils.data.random_split(\n            train_dataset, [train_size, val_size]\n        )\n    \n    if test_path.exists():\n        print(\"\\nTest set:\")\n        test_dataset = CombinedImageDataset(\n            str(test_path), \n            transform=transform,\n            split_direction=split_direction\n        )\n    else:\n        print(\"\\nTest set: Using validation set\")\n        test_dataset = val_dataset\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n                             shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n                           shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n                            shuffle=False, num_workers=num_workers)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UNetDown(nn.Module):\n    #Downsampling block\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UNetUp(nn.Module):\n    #Upsampling block\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(Generator, self).__init__()\n        \n        self.down1 = UNetDown(in_channels, 64, normalize=False)  # 128x128\n        self.down2 = UNetDown(64, 128)                           # 64x64\n        self.down3 = UNetDown(128, 256)                          # 32x32\n        self.down4 = UNetDown(256, 512)                          # 16x16\n        self.down5 = UNetDown(512, 512)                          # 8x8\n        self.down6 = UNetDown(512, 512)                          # 4x4\n        self.down7 = UNetDown(512, 512)                          # 2x2\n        self.down8 = UNetDown(512, 512, normalize=False)         # 1x1\n\n        \n        self.up1 = UNetUp(512, 512, dropout=0.5)      # 2x2\n        self.up2 = UNetUp(1024, 512, dropout=0.5)     # 4x4\n        self.up3 = UNetUp(1024, 512, dropout=0.5)     # 8x8\n        self.up4 = UNetUp(1024, 512)                  # 16x16\n        self.up5 = UNetUp(1024, 256)                  # 32x32\n        self.up6 = UNetUp(512, 128)                   # 64x64\n        self.up7 = UNetUp(256, 64)                    # 128x128\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),  # 256x256\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        \n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        \n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \n    def __init__(self, in_channels=6):  # 3 (input) + 3 (target/generated)\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels, 64, normalization=False),  # 128x128\n            *discriminator_block(64, 128),                                # 64x64\n            *discriminator_block(128, 256),                               # 32x32\n            *discriminator_block(256, 512),                               # 16x16\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)                   # 16x16 patches\n        )\n\n    def forward(self, img_A, img_B):\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(generator, discriminator, train_loader, optimizer_G, optimizer_D, \n                criterion_GAN, criterion_L1, lambda_L1, device, epoch):\n    generator.train()\n    discriminator.train()\n    \n    epoch_g_loss = 0\n    epoch_d_loss = 0\n    epoch_gan_loss = 0\n    epoch_l1_loss = 0\n    \n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} - Training')\n    for batch_idx, (real_A, real_B) in enumerate(pbar):\n        real_A = real_A.to(device)\n        real_B = real_B.to(device)\n        \n        batch_size = real_A.size(0)\n        valid = torch.ones((batch_size, 1, 16, 16), device=device, requires_grad=False)\n        fake = torch.zeros((batch_size, 1, 16, 16), device=device, requires_grad=False)\n        \n        optimizer_G.zero_grad()\n        fake_B = generator(real_A)\n        pred_fake = discriminator(real_A, fake_B)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        loss_L1 = criterion_L1(fake_B, real_B)\n        loss_G = loss_GAN + lambda_L1 * loss_L1\n        loss_G.backward()\n        optimizer_G.step()\n        \n        optimizer_D.zero_grad()\n        pred_real = discriminator(real_A, real_B)\n        loss_real = criterion_GAN(pred_real, valid)\n        pred_fake = discriminator(real_A, fake_B.detach())\n        loss_fake = criterion_GAN(pred_fake, fake)\n        loss_D = 0.5 * (loss_real + loss_fake)\n        loss_D.backward()\n        optimizer_D.step()\n        \n        epoch_g_loss += loss_G.item()\n        epoch_d_loss += loss_D.item()\n        epoch_gan_loss += loss_GAN.item()\n        epoch_l1_loss += loss_L1.item()\n        \n        if batch_idx % 10 == 0:\n            wandb.log({\n                'batch/g_loss': loss_G.item(),\n                'batch/d_loss': loss_D.item(),\n                'batch/gan_loss': loss_GAN.item(),\n                'batch/l1_loss': loss_L1.item(),\n                'batch/step': epoch * len(train_loader) + batch_idx\n            })\n        \n        pbar.set_postfix({\n            'G': f'{loss_G.item():.4f}',\n            'D': f'{loss_D.item():.4f}',\n            'L1': f'{loss_L1.item():.4f}'\n        })\n    \n    num_batches = len(train_loader)\n    return (epoch_g_loss/num_batches, epoch_d_loss/num_batches,\n            epoch_gan_loss/num_batches, epoch_l1_loss/num_batches)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(generator, discriminator, val_loader, criterion_GAN, \n             criterion_L1, lambda_L1, device, epoch):\n    generator.eval()\n    discriminator.eval()\n    \n    val_g_loss = 0\n    val_d_loss = 0\n    \n    with torch.no_grad():\n        for real_A, real_B in tqdm(val_loader, desc=f'Epoch {epoch+1} - Validation'):\n            real_A = real_A.to(device)\n            real_B = real_B.to(device)\n            \n            batch_size = real_A.size(0)\n            valid = torch.ones((batch_size, 1, 16, 16), device=device)\n            fake = torch.zeros((batch_size, 1, 16, 16), device=device)\n            \n            fake_B = generator(real_A)\n            pred_fake = discriminator(real_A, fake_B)\n            loss_GAN = criterion_GAN(pred_fake, valid)\n            loss_L1 = criterion_L1(fake_B, real_B)\n            loss_G = loss_GAN + lambda_L1 * loss_L1\n            \n            pred_real = discriminator(real_A, real_B)\n            loss_real = criterion_GAN(pred_real, valid)\n            pred_fake = discriminator(real_A, fake_B)\n            loss_fake = criterion_GAN(pred_fake, fake)\n            loss_D = 0.5 * (loss_real + loss_fake)\n            \n            val_g_loss += loss_G.item()\n            val_d_loss += loss_D.item()\n    \n    num_batches = len(val_loader)\n    return val_g_loss/num_batches, val_d_loss/num_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_results(generator, val_loader, device, epoch):\n    generator.eval()\n    \n    with torch.no_grad():\n        real_A, real_B = next(iter(val_loader))\n        real_A = real_A.to(device)\n        fake_B = generator(real_A)\n        \n        def denorm(x):\n            return (x + 1) / 2\n        \n        num_vis = min(4, len(real_A))\n        fig, axes = plt.subplots(3, num_vis, figsize=(4*num_vis, 12))\n        \n        for i in range(num_vis):\n            input_img = denorm(real_A[i]).cpu().permute(1, 2, 0).numpy()\n            axes[0, i].imshow(input_img)\n            axes[0, i].set_title(f'Input {i+1}')\n            axes[0, i].axis('off')\n            \n            gen_img = denorm(fake_B[i]).cpu().permute(1, 2, 0).numpy()\n            axes[1, i].imshow(gen_img)\n            axes[1, i].set_title(f'Generated {i+1}')\n            axes[1, i].axis('off')\n            \n            target_img = denorm(real_B[i]).cpu().permute(1, 2, 0).numpy()\n            axes[2, i].imshow(target_img)\n            axes[2, i].set_title(f'Ground Truth {i+1}')\n            axes[2, i].axis('off')\n        \n        plt.tight_layout()\n        wandb.log({f\"generations/epoch_{epoch+1}\": wandb.Image(fig), \"epoch\": epoch + 1})\n        plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_pix2pix(generator, discriminator, train_loader, val_loader, \n                  config, device, save_dir='checkpoints'):\n    os.makedirs(save_dir, exist_ok=True)\n    \n    optimizer_G = optim.Adam(generator.parameters(), \n                            lr=config['lr_generator'], \n                            betas=(0.5, 0.999))\n    optimizer_D = optim.Adam(discriminator.parameters(), \n                            lr=config['lr_discriminator'], \n                            betas=(0.5, 0.999))\n    \n    criterion_GAN = nn.MSELoss()\n    criterion_L1 = nn.L1Loss()\n    \n    best_val_loss = float('inf')\n    \n    start_time = time.time()\n    \n    for epoch in range(config['num_epochs']):\n        epoch_start = time.time()\n        \n        train_g, train_d, train_gan, train_l1 = train_epoch(\n            generator, discriminator, train_loader,\n            optimizer_G, optimizer_D,\n            criterion_GAN, criterion_L1, config['lambda_l1'],\n            device, epoch\n        )\n        \n        val_g, val_d = validate(\n            generator, discriminator, val_loader,\n            criterion_GAN, criterion_L1, config['lambda_l1'],\n            device, epoch\n        )\n        \n        epoch_time = time.time() - epoch_start\n        \n        wandb.log({\n            'epoch': epoch + 1,\n            'train/g_loss': train_g,\n            'train/d_loss': train_d,\n            'train/gan_loss': train_gan,\n            'train/l1_loss': train_l1,\n            'val/g_loss': val_g,\n            'val/d_loss': val_d,\n            'epoch_time': epoch_time\n        })\n        \n        print(f\"\\nEpoch [{epoch+1}/{config['num_epochs']}] | Time: {epoch_time:.1f}s\")\n        print(f\"  Train - G: {train_g:.4f}, D: {train_d:.4f}\")\n        print(f\"  Val   - G: {val_g:.4f}, D: {val_d:.4f}\")\n        \n        if val_g < best_val_loss:\n            best_val_loss = val_g\n            checkpoint = {\n                'epoch': epoch,\n                'generator_state_dict': generator.state_dict(),\n                'discriminator_state_dict': discriminator.state_dict(),\n                'val_loss': val_g,\n            }\n            torch.save(checkpoint, os.path.join(save_dir, 'best_pix2pix.pth'))\n            wandb.save(os.path.join(save_dir, 'best_pix2pix.pth'))\n            print(f\"  ✓ Saved best model\")\n        \n        if (epoch + 1) % 10 == 0:\n            visualize_results(generator, val_loader, device, epoch)\n    \n    total_time = time.time() - start_time\n    wandb.run.summary['best_val_loss'] = best_val_loss\n    wandb.run.summary['total_training_time'] = total_time\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"Training completed in {total_time/3600:.2f} hours\")\n    print(\"=\"*70)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BallDetector:\n    \n    def __init__(self, ball_radius=15, min_radius=10, max_radius=25):\n        self.ball_radius = ball_radius\n        self.min_radius = min_radius\n        self.max_radius = max_radius\n    \n    def detect_ball(self, image):\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = image\n        \n        blurred = cv2.GaussianBlur(gray, (9, 9), 2)\n        circles = cv2.HoughCircles(\n            blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=50,\n            param1=50, param2=30,\n            minRadius=self.min_radius, maxRadius=self.max_radius\n        )\n        \n        if circles is None or len(circles[0]) == 0:\n            return None\n        \n        circle = circles[0][0]\n        return {'position': (circle[0], circle[1]), 'radius': circle[2]}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_pix2pix(generator, test_loader, device):\n    generator.eval()\n    detector = BallDetector()\n    \n    all_errors_x = []\n    all_errors_y = []\n    failed_predictions = 0\n    total_predictions = 0\n    sample_predictions = []\n    \n    print(\"Evaluating Pix2Pix Model\")\n    \n    with torch.no_grad():\n        for batch_idx, (real_A, real_B) in enumerate(tqdm(test_loader, desc='Evaluating')):\n            real_A = real_A.to(device)\n            fake_B = generator(real_A)\n            \n            fake_imgs = ((fake_B + 1) / 2 * 255).cpu().numpy().astype(np.uint8)\n            real_imgs = ((real_B + 1) / 2 * 255).numpy().astype(np.uint8)\n            \n            for i in range(len(fake_imgs)):\n                total_predictions += 1\n                \n                pred = fake_imgs[i].transpose(1, 2, 0)\n                target = real_imgs[i].transpose(1, 2, 0)\n                \n                # Log first 10 samples\n                if len(sample_predictions) < 10:\n                    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n                    axes[0].imshow(pred)\n                    axes[0].set_title('Prediction')\n                    axes[0].axis('off')\n                    axes[1].imshow(target)\n                    axes[1].set_title('Ground Truth')\n                    axes[1].axis('off')\n                    plt.tight_layout()\n                    sample_predictions.append(wandb.Image(fig))\n                    plt.close()\n                \n                pred_ball = detector.detect_ball(pred)\n                target_ball = detector.detect_ball(target)\n                \n                if pred_ball is None or target_ball is None:\n                    failed_predictions += 1\n                    continue\n                \n                pos_error_x = abs(pred_ball['position'][0] - target_ball['position'][0])\n                pos_error_y = abs(pred_ball['position'][1] - target_ball['position'][1])\n                \n                all_errors_x.append(pos_error_x)\n                all_errors_y.append(pos_error_y)\n    \n    failure_rate = (failed_predictions / total_predictions) * 100\n    \n    results = {\n        'test/position_x_mean': np.mean(all_errors_x) if len(all_errors_x) > 0 else 0,\n        'test/position_x_std': np.std(all_errors_x) if len(all_errors_x) > 0 else 0,\n        'test/position_y_mean': np.mean(all_errors_y) if len(all_errors_y) > 0 else 0,\n        'test/position_y_std': np.std(all_errors_y) if len(all_errors_y) > 0 else 0,\n        'test/failure_rate': failure_rate,\n        'test/total_predictions': total_predictions,\n        'test/failed_predictions': failed_predictions\n    }\n    \n    wandb.log(results)\n    wandb.log({\"test/sample_predictions\": sample_predictions})\n    \n    comparison_table = wandb.Table(\n        columns=[\"Metric\", \"Paper (Pix2Pix)\", \"Our Results\"],\n        data=[\n            [\"Position X Error (pixels)\", \"6.28 ± 7.98\", f\"{results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f}\"],\n            [\"Position Y Error (pixels)\", \"11.7 ± 12.8\", f\"{results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f}\"],\n            [\"Failure Rate (%)\", \"7%\", f\"{failure_rate:.2f}%\"]\n        ]\n    )\n    wandb.log({\"comparison_with_paper\": comparison_table})\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EVALUATION RESULTS\")\n    print(\"=\"*70)\n    print(f\"Total: {total_predictions}, Failed: {failed_predictions} ({failure_rate:.2f}%)\")\n    \n    if len(all_errors_x) > 0:\n        print(f\"\\nPosition X Error: {results['test/position_x_mean']:.2f} ± {results['test/position_x_std']:.2f} pixels\")\n        print(f\"Position Y Error: {results['test/position_y_mean']:.2f} ± {results['test/position_y_std']:.2f} pixels\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"COMPARISON WITH PAPER RESULTS (Table 11)\")\n    print(\"=\"*70)\n    print(\"Paper Pix2Pix Results:\")\n    print(\"  Position X: 6.28 ± 7.98 pixels\")\n    print(\"  Position Y: 11.7 ± 12.8 pixels\")\n    print(\"  Rotation: 17.2 ± 20.8 degrees\")\n    print(\"  Roundness: 0.56 ± 0.14 pixels\")\n    print(\"  Failure Rate: 7%\")\n    print(\"  Number of balls: 93% single ball, 7% no/multiple balls\")\n    print(\"=\"*70)\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(use_wandb=True, wandb_project=\"physicsgen-pix2pix\", wandb_entity=None):\n    DATA_ROOT = '/kaggle/input/ball-data/ball_bounce/ball_bounce'\n    \n    CONFIG = {\n        'batch_size': 18,\n        'lr_discriminator': 1e-4,\n        'lr_generator': 2e-4,\n        'num_epochs': 20,\n        'lambda_l1': 100,\n        'image_size': 256,\n        'num_workers': 2,\n        'split_direction': 'horizontal',\n    }\n    \n    if use_wandb:\n        wandb.init(\n            project=wandb_project,\n            entity=wandb_entity,\n            config=CONFIG,\n            name=f\"pix2pix-combined-images\",\n            tags=['pix2pix', 'combined-images', 'physicsgen']\n        )\n        config = wandb.config\n    else:\n        config = CONFIG\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    train_loader, val_loader, test_loader = create_data_loaders(\n        DATA_ROOT,\n        batch_size=config['batch_size'],\n        image_size=config['image_size'],\n        num_workers=config['num_workers'],\n        split_direction=config['split_direction']\n    )\n    \n    generator = Generator().to(device)\n    discriminator = Discriminator().to(device)\n    \n    print(f\"\\nGenerator params: {sum(p.numel() for p in generator.parameters()):,}\")\n    print(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")\n    \n    if use_wandb:\n        wandb.watch(generator, log='all', log_freq=100)\n        wandb.watch(discriminator, log='all', log_freq=100)\n    \n    train_pix2pix(generator, discriminator, train_loader, val_loader, config, device)\n    \n    if use_wandb:\n        wandb.finish()\n    \n    return generator, discriminator\n\n\nif __name__ == '__main__':\n    DATA_ROOT = '/kaggle/input/ball-data/ball_bounce'\n    USE_WANDB = True\n    WANDB_PROJECT = \"physicsgen-pix2pix\"\n    \n    generator, discriminator = main(\n        use_wandb=USE_WANDB,\n        wandb_project=WANDB_PROJECT\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}